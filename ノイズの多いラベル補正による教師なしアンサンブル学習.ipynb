{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "収束しました。\n",
      "使用するシード: 2022\n",
      "収束しました。\n",
      "\n",
      "Alpha: 0.7\n",
      "収束しました。\n",
      "統合ラベルの評価:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000     29492\n",
      "           1     1.0000    1.0000    1.0000     30508\n",
      "\n",
      "    accuracy                         1.0000     60000\n",
      "   macro avg     1.0000    1.0000    1.0000     60000\n",
      "weighted avg     1.0000    1.0000    1.0000     60000\n",
      "\n",
      "修正後のラベルの評価:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9804    0.9498    0.9649     29492\n",
      "           1     0.9529    0.9817    0.9671     30508\n",
      "\n",
      "    accuracy                         0.9660     60000\n",
      "   macro avg     0.9667    0.9657    0.9660     60000\n",
      "weighted avg     0.9664    0.9660    0.9660     60000\n",
      "\n",
      "修正されたインスタンスラベルの数: 2040\n",
      "強化された統合ラベルセットの評価結果:\n",
      "   alpha  f1_score\n",
      "0    0.1  0.866386\n",
      "1    0.2  0.866386\n",
      "2    0.3  0.866386\n",
      "3    0.4  0.866386\n",
      "4    0.5  0.866386\n",
      "5    0.6  0.866386\n",
      "6    0.7  0.866386\n",
      "7    0.8  0.866386\n",
      "8    0.9  0.866386\n",
      "9    1.0  0.866386\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# EMアルゴリズム\n",
    "def em_algorithm(predicted_labels, num_classes, num_iterations=100, tol=1e-4, random_seed=0):\n",
    "    num_instances, num_classifiers = predicted_labels.shape\n",
    "    np.random.seed(random_seed)\n",
    "    Pr = np.random.dirichlet(np.ones(num_classes))\n",
    "    Pi = np.random.dirichlet(np.ones(num_classes), size=(num_classifiers, num_classes))\n",
    "    log_likelihoods = []\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        likelihood = np.zeros((num_instances, num_classes))\n",
    "        for k in range(num_classes):\n",
    "            prod_terms = np.ones(num_instances)\n",
    "            for j in range(num_classifiers):\n",
    "                prod_terms *= Pi[j, k, predicted_labels[:, j].astype(int)]\n",
    "            likelihood[:, k] = Pr[k] * prod_terms\n",
    "        likelihood_sum = likelihood.sum(axis=1, keepdims=True)\n",
    "        likelihood /= likelihood_sum\n",
    "\n",
    "        log_likelihood = np.sum(np.log(np.maximum(likelihood_sum, 1e-10)))\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "\n",
    "        if iteration > 0 and np.abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "            print(\"収束しました。\")\n",
    "            break\n",
    "        \n",
    "        Pr = likelihood.mean(axis=0)\n",
    "        for j in range(num_classifiers):\n",
    "            for k in range(num_classes):\n",
    "                Pi[j, k, :] = np.dot(likelihood[:, k], (predicted_labels[:, j] == np.arange(num_classes).reshape(-1, 1)).T)\n",
    "                Pi[j, k, :] /= np.maximum(likelihood[:, k].sum(), 1e-10)\n",
    "    \n",
    "    return likelihood.argmax(axis=1), log_likelihoods\n",
    "\n",
    "# 第一段階のフィルタリング\n",
    "def first_stage_filtering(X, integrated_labels, num_folds=7):\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    high_quality_datasets = []\n",
    "    suspected_datasets = []\n",
    "    models = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = integrated_labels[train_index], integrated_labels[test_index]\n",
    "\n",
    "        model = AdaBoostClassifier(n_estimators=10, algorithm='SAMME.R')\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        models.append(model)\n",
    "\n",
    "        high_quality_set = test_index[y_pred == y_test]\n",
    "        suspected_set = test_index[y_pred != y_test]\n",
    "\n",
    "        high_quality_datasets.append(high_quality_set)\n",
    "        suspected_datasets.append(suspected_set)\n",
    "\n",
    "    high_quality_final = np.concatenate(high_quality_datasets)\n",
    "    suspected_final = np.concatenate(suspected_datasets)\n",
    "\n",
    "    return high_quality_final, suspected_final, models\n",
    "\n",
    "# 第二段階のフィルタリング\n",
    "def second_stage_filtering(X, integrated_labels, high_quality_final, suspected_final, models, alpha=0.7):\n",
    "    M = len(models)\n",
    "    Si = np.array([np.sum([integrated_labels[idx] != integrated_labels[model.predict(X[idx].reshape(1, -1))[0]] for model in models]) for idx in suspected_final])\n",
    "    prob_product = np.array([np.prod([model.predict_proba(X[idx].reshape(1, -1))[0][integrated_labels[idx]] for model in models]) for idx in suspected_final])\n",
    "    denominator = np.sum(prob_product)\n",
    "    Ri = prob_product / np.maximum(denominator, 1e-10) + Si\n",
    "\n",
    "    sorted_indices = np.argsort(Ri)[::-1]\n",
    "    top_alpha = int(alpha * len(sorted_indices))\n",
    "    tough_instances = suspected_final[sorted_indices[:top_alpha]]\n",
    "\n",
    "    high_quality_final = np.intersect1d(high_quality_final, np.unique(np.concatenate([model.predict(X[high_quality_final]) for model in models])))\n",
    "\n",
    "    return tough_instances, high_quality_final\n",
    "\n",
    "# ラベルの更新\n",
    "def update_labels(X, tough_instances, high_quality_final, integrated_labels):\n",
    "    high_quality_data = X[high_quality_final]\n",
    "    high_quality_labels = integrated_labels[high_quality_final]\n",
    "    correcting_models = []\n",
    "\n",
    "    for _ in range(10):  # Use 10 models for correction\n",
    "        model = AdaBoostClassifier(n_estimators=50, algorithm='SAMME.R')\n",
    "        model.fit(high_quality_data, high_quality_labels)\n",
    "        correcting_models.append(model)\n",
    "\n",
    "    corrected_labels = np.copy(integrated_labels)\n",
    "    for idx in tough_instances:\n",
    "        votes = np.array([model.predict(X[idx].reshape(1, -1))[0] for model in correcting_models])\n",
    "        if np.bincount(votes).max() != len(votes):  # すべてのモデルが同じラベルを予測しない場合にのみ修正\n",
    "            corrected_labels[idx] = np.argmax(np.bincount(votes))\n",
    "\n",
    "    return corrected_labels\n",
    "\n",
    "# 再度EMアルゴリズムを適用\n",
    "def reapply_em_algorithm(X, corrected_labels, tough_instances, models):\n",
    "    tough_labels = np.zeros((len(tough_instances), len(models)), dtype=int)\n",
    "    for i, model in enumerate(models):\n",
    "        tough_labels[:, i] = model.predict(X[tough_instances])\n",
    "    \n",
    "    new_labels, _ = em_algorithm(tough_labels, num_classes=2)\n",
    "    corrected_labels[tough_instances] = new_labels\n",
    "    \n",
    "    return corrected_labels\n",
    "\n",
    "# 5回の7-foldクロスバリデーションを実行\n",
    "def repeated_kfold_cross_validation(X, integrated_labels, num_repeats=5, num_folds=7):\n",
    "    all_high_quality = []\n",
    "    all_suspected = []\n",
    "    all_models = []\n",
    "\n",
    "    for _ in range(num_repeats):\n",
    "        high_quality_final, suspected_final, models = first_stage_filtering(X, integrated_labels, num_folds)\n",
    "        all_high_quality.append(high_quality_final)\n",
    "        all_suspected.append(suspected_final)\n",
    "        all_models.extend(models)\n",
    "\n",
    "    high_quality_final = np.concatenate(all_high_quality)\n",
    "    suspected_final = np.concatenate(all_suspected)\n",
    "\n",
    "    return high_quality_final, suspected_final, all_models\n",
    "\n",
    "# MNISTデータセットのロードと前処理\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# データの標準化\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 2クラス分類に変換\n",
    "y = np.where(y % 2 == 0, 0, 1)\n",
    "\n",
    "# データをトレーニングセットとテストセットに分割\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# 初期ラベルの予測 (DecisionTreeClassifierを使用)\n",
    "num_classifiers = 3\n",
    "predicted_labels = np.zeros((X_train.shape[0], num_classifiers), dtype=int)\n",
    "\n",
    "# 基本モデルとしてDecisionTreeを使用\n",
    "models = []\n",
    "for i in range(num_classifiers):\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(X_train, y_train)\n",
    "    predicted_labels[:, i] = dt.predict(X_train)\n",
    "    models.append(dt)\n",
    "\n",
    "# テストデータの分類\n",
    "test_predictions = np.zeros((X_test.shape[0], num_classifiers), dtype=int)\n",
    "for i in range(num_classifiers):\n",
    "    test_predictions[:, i] = models[i].predict(X_test)\n",
    "\n",
    "# テストデータの統合ラベル推定\n",
    "test_integrated_labels, _ = em_algorithm(test_predictions, num_classes=2, random_seed=2022)\n",
    "\n",
    "# 複数の初期化方法を試す\n",
    "best_seed = 2022\n",
    "alpha = 0.7  # 一つのα値のみを試す\n",
    "\n",
    "print(f\"使用するシード: {best_seed}\")\n",
    "integrated_labels, log_likelihoods = em_algorithm(predicted_labels, num_classes=2, random_seed=best_seed)\n",
    "\n",
    "high_quality_final, suspected_final, models = repeated_kfold_cross_validation(X_train, integrated_labels)\n",
    "\n",
    "print(f\"\\nAlpha: {alpha}\")\n",
    "tough_instances, high_quality_final = second_stage_filtering(X_train, integrated_labels, high_quality_final, suspected_final, models, alpha=alpha)\n",
    "corrected_labels = update_labels(X_train, tough_instances, high_quality_final, integrated_labels)\n",
    "corrected_labels = reapply_em_algorithm(X_train, corrected_labels, tough_instances, models)\n",
    "\n",
    "integrated_accuracy = accuracy_score(y_train, integrated_labels)\n",
    "corrected_accuracy = accuracy_score(y_train, corrected_labels)\n",
    "\n",
    "print(\"統合ラベルの評価:\")\n",
    "print(classification_report(y_train, integrated_labels, digits=4))\n",
    "print(\"修正後のラベルの評価:\")\n",
    "print(classification_report(y_train, corrected_labels, digits=4))\n",
    "\n",
    "# 修正されたインスタンスラベルの数を出力\n",
    "num_corrected_labels = np.sum(corrected_labels != integrated_labels)\n",
    "print(f\"修正されたインスタンスラベルの数: {num_corrected_labels}\")\n",
    "\n",
    "# 強化された統合ラベルセットの評価\n",
    "def evaluate_enhanced_labels(X, y, corrected_labels, alpha_values):\n",
    "    results = []\n",
    "    for alpha in alpha_values:\n",
    "        kf = KFold(n_splits=7)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = corrected_labels[train_index], y[test_index]\n",
    "            \n",
    "            model = AdaBoostClassifier(n_estimators=50, algorithm='SAMME.R')\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        results.append({\n",
    "            'alpha': alpha,\n",
    "            'f1_score': np.mean(f1_scores)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# α値の範囲を指定して評価を実行\n",
    "alpha_values = np.arange(0.1, 1.1, 0.1)\n",
    "evaluation_results = evaluate_enhanced_labels(X_train, y_train, corrected_labels, alpha_values)\n",
    "\n",
    "print(\"強化された統合ラベルセットの評価結果:\")\n",
    "print(evaluation_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
